<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <title>Machine Learning</title>
    <link rel="stylesheet" href="styles.css">
</head>

<body>
    <h1>Machine Learning</h1>

    <!-- Navigation Bar -->
    <nav>
        <table class="nav-list">
            <td><a href="index.html">Home</a></td>
            <td><a href="aboutus.html">About Us</a></td>
            <td><a href="resources.html">Resources</a></td>
            <td><a href="techhero.html">Tech Heroes</a></td>
            <td><a href="ourmachine.html">Machine Learning</a></td>
            
        </table>
    </nav>

    <nav>   <!-- Sub Navigation for Machine Learning Page -->

        <a href="ourmachine.html"><button class="MLA-But">Try Our Model</button></a>
        <a href="Objective.html"> <button class="MLA-But">Objective</button></a>
        <a href="Process.html"> <button class="MLA-But">Process</button></a>
        <a href="BuolamwinisWork.html"><button class="MLA-But">Buolamwini's Work</button></a>
        <a href="LessonsLearned.html"> <button class="MLA-But">Lessons Learned</button></a>
        <a href="MLvideo.html"><button class="MLA-But">Video Example</button></a>

    </nav>
    <br>
   
    <div class="ML-Words">  <!--Description of our Process-->
         <h2>Process</h2>
    <br>
    <p>Our process began with defining what kind of model we would train. We considered building a model around
        sound but ultimately decided on a visual one, reasoning that it would be easier to connect back to our
        coursework on visual bias—specifically racial, appearance, and body presentation bias. While we did talk
        about biases also existing in how people sound (e.g., accent, tone), a visual model allowed us to study
        bias in a more tangible, visual way.</p>
    <div class="image-pair"> <!--Images of Origami Balls-->
        <img src="Black and White Origami Balls.jpg" alt="Black/White Origami Balls">
        <img src="ColorOrigamiBalls.jpg" alt="Color Origami Balls">
    </div>
    
    <p>Initially, our model was trained on just two classes of black-and-white origami balls versus colorful origami balls. However, in response to thoughtful critiques and a desire to better reflect the themes in Unmasking AI, we rebuild our approach to go beyond binary classifications.
</p>

	    <p>We expanded our dataset to include six distinct object classes: Black/White Origami Balls, Colorful Origami Balls, Crumpled Paper, Ping-pong Balls (for Smooth Texture), Origami Boxes, Blank Sheets of Paper</p>  <!--List of objects included in our datsets-->

	  
	    <img src = "machinelearningobjects.png" alt = "Machine Learning Objects Model" style = "width: 80%; border-radius: 10px; display: block; margin 0 auto; height: auto;">


	

	    <p>This variety of objects allows us to explore how the model perceives form, color, and material and where it starts to struggle. For instance, how does it distinguish between an orange ping-pong ball and a colored origami ball? Does it confuse a folded paper box with a ball if both are colorful? These experiments mirrored real-world scenarios where systems break down at the edges or misread context entirely.
The technical side of training the model was relatively smooth using Teachable Machine, but integrating it into our website by using HTML and JavaScript required trial and error. Although we leaned on the tutorial code, connecting the model and making it function dynamically on the webpage demanded additional debugging.
</p>
    </div>
    <footer>
		© <Span><script type="text/JavaScript">document.write(new Date().getFullYear());</script></Span>
		   LIS500, Inc. All Rights Reserved.
		    <br>
			<br>
		    Nsikan Morgan  |  Rachel Ren 
	</footer>
</body>
</html>
