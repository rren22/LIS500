<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <title>Machine Learning</title>
    <link rel="stylesheet" href="styles.css">
</head>

<body>
    <h1>Machine Learning</h1>

    <!-- Navigation Bar -->
    <nav>
        <table class="nav-list">
            <td><a href="index.html">Home</a></td>
            <td><a href="aboutus.html">About Us</a></td>
            <td><a href="resources.html">Resources</a></td>
            <td><a href="techhero.html">Tech Heroes</a></td>
            <td><a href="ourmachine.html">Machine Learning</a></td>
            
        </table>
    </nav>

    <nav>

        <a href="ourmachine.html"><button class="MLA-But">Try Our Model</button></a>
        <a href="Objective.html"> <button class="MLA-But">Objective</button></a>
        <a href="Process.html"> <button class="MLA-But">Process</button></a>
        <a href="BuolamwinisWork.html"><button class="MLA-But">Buolamwini's Work</button></a>
        <a href="LessonsLearned.html"> <button class="MLA-But">Lessons Learned</button></a>
        <a href="MLvideo.html"><button class="MLA-But">Video Example</button></a>

    </nav>
    <br>
   
    
    <div class="ML-Words">
      <h2>Connecting to Buolamwini's Work</h2>
    <br>  
    <p>Buolamwini’s research provided a powerful framework for critically reflecting on our model. In Unmasking
        AI, she recounts her experience with facial recognition software that failed to accurately identify
        darker-skinned women while performing much better on lighter-skinned men. This was due to bias in the
        training data, which overwhelmingly featured lighter-skinned male faces. As a result, the algorithm
        began to categorize those faces as "default." That made sense by what we observed in our own experiment:
        the model only learned from examples that we taught it.</p>
    <p>If we had trained the model with fewer images or excluded edge cases—like origami balls of mixed color or
        under different lighting—it would have misclassified those inputs. This is the way AI models operate in
        practice, such as recruitment software, and can perpetuate systemic discrimination by accident. Suppose,
        for instance, a company trains a model on data solely from "ideal" candidates holding college degrees
        and conventional résumés. The model may eventually choose mostly white or male candidates—proof of
        current disparities rather than reducing them.</p>
    </div>
    <footer>
		© <Span><script type="text/JavaScript">document.write(new Date().getFullYear());</script></Span>
		   LIS500, Inc. All Rights Reserved.
		    <br>
			<br>
		    Nsikan Morgan  |  Rachel Ren 
	</footer>

    </body>
    </html>
