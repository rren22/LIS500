<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>Machine Learning</title>
    <link rel="stylesheet" href="styles.css">
</head>

<body>
    <h1>Machine Learning</h1>

    <!-- Navigation Bar -->
    <nav>
        <table class="nav-list">
            <td><a href="index.html">Home</a></td>
            <td><a href="aboutus.html">About Us</a></td>
            <td><a href="resources.html">Resources</a></td>
            <td><a href="techhero.html">Tech Heroes</a></td>
            <td><a href="machinelearning.html">Machine Learning</a></td>
            <td><a href="ourmachine.html">Try Our Model</a></td>
        </table>
    </nav>

  <div>
      <h2>Machine Learning Application</h2>
      <section class = "project3">
          <h3>Project Objective & Scope</h3>
               <p>Our team, for our machine learning project, aimed to train a model with Google's <strong>Teachable Machine</strong> to differentiate between black-and-white origami balls and colored origami balls. This rather straightforward visual task enabled us to examine the degree to which accessible tools such as Teachable Machine can facilitate image classification without requiring deep programming skills.</p>
               <p>But while our technical goal was straightforward—detecting color differences—the larger lesson arrived as we wrestled with the social and ethical consequences of algorithmic decision-making. Reading <em>Unmasking AI</em> by Joy Buolamwini alongside this project expanded our understanding of what it means to design algorithms in a society that is already biased. Buolamwini's research reveals the dangers of AI systems that perpetuate racial biases due to the data upon which they are trained. We began to understand that even the most "neutral" AI systems are programmed by a set of choices: what information we give them, how we label that information, and what outcomes we aim to optimize for.</p>
          <h3>Process</h3>
              <p>Our process began with defining what kind of model we would train. We considered building a model around sound but ultimately decided on a visual one, reasoning that it would be easier to connect back to our coursework on visual bias—specifically racial, appearance, and body presentation bias. While we did talk about biases also existing in how people sound (e.g., accent, tone), a visual model allowed us to study bias in a more tangible, visual way.</p>
              <p>Once we had chosen a visual model, we had to choose what objects to scan. In contrast to the tutorial video illustration, we didn't have many choices readily at hand—so we chose two origami balls: one colored, and one black and white. We trained the Teachable Machine to recognize the visual difference between the two. While the training process itself was straightforward, installing the model onto our website took additional time and debugging. We just copied the tutorial code but had to use additional resources to properly interface our trained model and have it show up and work on the webpage. We managed to finally get it to run successfully by trial and error.</p>
          <h3>Connecting to Buolamwini's Work</h3>
              <p>Buolamwini’s research provided a powerful framework for critically reflecting on our model. In Unmasking AI, she recounts her experience with facial recognition software that failed to accurately identify darker-skinned women while performing much better on lighter-skinned men. This was due to bias in the training data, which overwhelmingly featured lighter-skinned male faces. As a result, the algorithm began to categorize those faces as "default." That made sense by what we observed in our own experiment: the model only learned from examples that we taught it.</p>
              <p>If we had trained the model with fewer images or excluded edge cases—like origami balls of mixed color or under different lighting—it would have misclassified those inputs. This is the way AI models operate in practice, such as recruitment software, and can perpetuate systemic discrimination by accident. Suppose, for instance, a company trains a model on data solely from "ideal" candidates holding college degrees and conventional résumés. The model may eventually choose mostly white or male candidates—proof of current disparities rather than reducing them.</p>
          <h3>Lesson Learned</h3>
               <p>Teachable Machine made it simple to build and export a working model, and that is proof of the increased accessibility of machine learning tools. But this also presents enormous ethical responsibilities. Just because anyone can build an algorithm doesn't necessarily mean every algorithm is built with care or in an inclusive way.</p>
               <p>One significant thing we learned is that technology is not value-neutral. Algorithms reflect the values, assumptions, and blind spots of their developers. For fairness, diversity at the development stage—the people who develop the tools and the data we train on—is key. Ethical concerns must be incorporated into the development process from the very beginning of AI development so that these systems do not harm unintentionally the very people they are designed to serve.</p>
               <p>Personally, the biggest moment of surprise was realizing just how easily our model would mislabel something. If we held in front of the camera a black-and-white object that was not an origami ball, the model would refuse to classify it otherwise. This revealed to us just how literal and one-dimensional the machine's "comprehension" is—it doesn't know what an origami ball is, only what pixel configurations it was shown during training.</p>
               <p>This has deep implications in daily life. Whether it's a facial recognition program that misidentifies darker-skinned people, or a hiring algorithm that learns from biased résumés, AI will mirror the gaps, omissions, and biases of its training data unless we intervene.</p>
          <h3>Conclusion</h3>
               <p>By this project, we learned about the potentiality and vulnerability of machine learning systems. In spite of the seemingly objective and unproblematic task—sorting colored from black-and-white origami balls—we noticed how bias and design came into play. Reading <em>Unmasking AI</em> alongside this technical process helped us see that developing fair and equitable technology is more than technical expertise; it is something that requires reflection, responsibility, and ongoing care about equity.</p>
      </section>
      <h2>
    Video Example
</h2>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/lmQcyhHgY8E?si=mBQ3iFGvOBL6PgEU" 
        title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
        referrerpolicy="strict-origin-when-cross-origin" allowfullscreen>
        
    </iframe>
  </div>

</body>
</html>
