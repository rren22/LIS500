<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <title>Machine Learning</title>
    <link rel="stylesheet" href="styles.css">
</head>

<body>
    <h1>Machine Learning</h1>

    <!-- Navigation Bar -->
    <nav>
        <table class="nav-list">
            <td><a href="index.html">Home</a></td>
            <td><a href="aboutus.html">About Us</a></td>
            <td><a href="resources.html">Resources</a></td>
            <td><a href="techhero.html">Tech Heroes</a></td>
            <td><a href="ourmachine.html">Machine Learning</a></td>
            
        </table>
    </nav>

    <nav>   <!-- Sub Navigation for Machine Learning Page-->

        <a href="ourmachine.html"><button class="MLA-But">Try Our Model</button></a>
        <a href="Objective.html"> <button class="MLA-But">Objective</button></a>
        <a href="Process.html"> <button class="MLA-But">Process</button></a>
        <a href="BuolamwinisWork.html"><button class="MLA-But">Buolamwini's Work</button></a>
        <a href="LessonsLearned.html"> <button class="MLA-But">Lessons Learned</button></a>
        <a href="MLvideo.html"><button class="MLA-But">Video Example</button></a>

    </nav>
    <br>
 
    
    <div class="ML-Words">  <!-- Mixed of what we learned while reading Joy's Book-->
       <h2>Lesson Learned</h2>
    <br> 
    <p>Teachable Machine made it simple to build and export a working model, and that is proof of the increased accessibility of machine learning tools. But this also presents enormous ethical responsibilities. As Buolamwini reminds us, “AI will not solve discrimination, because the cultural patterns that say one group of people is better than another… are not technical” (Buolamwini 15). Just because anyone can build an algorithm doesn't necessarily mean every algorithm is built with care or in an inclusive way.
</p>
    <p>One significant thing we learned is that technology is not value-neutral. Algorithms reflect the values, assumptions, and blind spots of their developers. For fairness, diversity at the development stage—the people who develop the tools and the data we train on—is key. Ethical concerns must be incorporated into the development process from the very beginning of AI development so that these systems do not unintentionally harm the very people they are designed to serve.
</p>
    <p>Our work also showed the gap between representation and equity. While we expanded our objects into our dataset, we realized that inclusion is not enough. We need contextual awareness and critical thinking that must guide design choices. As Buolamwini noted, “There are costs of inclusion and cost of exclusion to be considered in the design and deployment of AI systems that must be contextualized” (Buolamwini 258). By including more objects in our dataset could make our model appear more “complete,” but if those additions were made without thoughtfulness, if they weren’t clearly distinguishable, sufficiently represented, or tied back to the ethical stakes of our project, we risked introducing confusion instead of clarity.
</p>
    <p>Personally, the biggest moment of surprise was realizing just how easily our model would mislabel something. If we held in front of the camera a black-and-white object that was not an origami ball, the model would refuse to classify it otherwise. As Buolamwini puts it, “The machines we build reflect the priorities, preferences, and even prejudices of those who have the power to shape technology” (Buolamwini 69). This revealed to us just how literal and one-dimensional the machine's "comprehension" is—it doesn't know what an origami ball is, only what pixel configurations it was shown during training.
</p>
    <p>This has deep implications in daily life. Whether it's a facial recognition program that misidentifies
        darker-skinned people, or a hiring algorithm that learns from biased résumés, AI will mirror the gaps,
        omissions, and biases of its training data unless we intervene.</p>

    <h2>Conclusion</h2> <!-- Conclusion Section-->
        <p>This project began as a technical challenge and became an exploration of the social consequences of classification. With guidance from Unmasking AI, and foundational ideas from intersectionality and implicit bias resources, we came to understand that machine learning systems have many different forms. 
If we want AI systems that are more inclusive and just, we should recognize that every model begins with a choice of whose experiences will be represented? Whose experiences will be ignored? Building a justified technology is not just a matter of code, but how we want them to be shaped.
</p>
        </div>
        <footer>
            © <Span><script type="text/JavaScript">document.write(new Date().getFullYear());</script></Span>
               LIS500, Inc. All Rights Reserved.
                <br>
                <br>
                Nsikan Morgan  |  Rachel Ren 
        </footer>
</body>
</html>
    
    
